{"cells":[{"cell_type":"code","execution_count":null,"id":"29b387b0","metadata":{"id":"29b387b0"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    roc_auc_score, confusion_matrix\n",")\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import os\n","from PIL import Image\n","from tqdm import tqdm\n","import json\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":null,"id":"7e962aa5","metadata":{"id":"7e962aa5"},"outputs":[],"source":["class ConfidenceWeightedVoting():\n","    def __init__(self, n_classes):\n","        super().__init__()\n","        self.n_classes = n_classes\n","\n","    def forward(self, instance_logits):\n","        # get probabilities\n","        instance_probs = torch.softmax(instance_logits, dim=-1)\n","\n","        # get predictions and confidences\n","        instance_predictions = torch.argmax(instance_probs, dim=-1)\n","        instance_confidences = torch.max(instance_probs, dim=-1)[0]\n","\n","        # use majority voting for each prediction weighted by confidence\n","        bag_logits = torch.zeros(self.n_classes, device=instance_logits.device)\n","        for pred, conf in zip(instance_predictions, instance_confidences):\n","            bag_logits[pred] += conf\n","\n","        instance_info = {\n","            \"predictions\": instance_predictions,\n","            \"confidences\": instance_confidences,\n","            \"probabilities\": instance_probs,\n","        }\n","\n","        return bag_logits, instance_info"]},{"cell_type":"code","execution_count":null,"id":"470330ad","metadata":{"id":"470330ad"},"outputs":[],"source":["class MIL_FabricClassifier(nn.Module):\n","    def __init__(self, n_classes, pretrained_path=None, agg_type='confidence_voting'):\n","        super().__init__()\n","\n","        # load pretrained resnet18 model\n","        self.instance_model = models.resnet18(pretrained=False)\n","        checkpoint = torch.load(pretrained_path, map_location='cpu')\n","        state_dict = checkpoint.get('model_state_dict', checkpoint)\n","        state_dict = {k: v for k, v in state_dict.items() if 'fc' not in k}\n","        self.instance_model.load_state_dict(state_dict, strict=False)\n","\n","        # replace final classifier to outut 7 classes\n","        self.instance_model.fc = nn.Linear(512, n_classes)\n","\n","        self.n_classes = n_classes\n","\n","        self.agg_type = agg_type\n","        if agg_type == 'confidence_voting':\n","            self.agg = ConfidenceWeightedVoting(n_classes)\n","\n","    def forward(self, bag_dict):\n","        bag_logits_list = []\n","        bag_pred_list = []\n","        bag_info_dict = {}\n","        bag_ids = []\n","\n","        for bag_id, instances in bag_dict.items():\n","            instance_logits = self.instance_model(instances.float())\n","\n","            # MIL aggregation\n","            bag_logits, instance_info = self.agg(instance_logits)\n","\n","            bag_prediction = torch.argmax(bag_logits)\n","\n","            bag_output_list.append(votes)\n","            bag_ids.append(bag_id)\n","\n","            bag_logits_list.append(bag_logits)\n","            bag_pred_list.append(bag_prediction)\n","            bag_ids.append(bag_id)\n","\n","            bag_info_dict[bag_id] = {\n","                'instance_predictions': instance_info['predictions'].cpu(),\n","                'instance_confidences': instance_info['confidences'].cpu(),\n","                'instance_probabilities': instance_info['probabilities'].cpu()\n","            }\n","\n","        bag_logits_batch = torch.stack(bag_logits_list)\n","        bag_pred_batch = torch.stack(bag_pred_list)\n","\n","        return bag_logits_batch, bag_pred_batch, bag_info_dict, bag_ids\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f05477a4","metadata":{"id":"f05477a4"},"outputs":[],"source":["class FabricMILDataset(Dataset):\n","    def __init__(self, data_dict, transform=None):\n","        self.data_dict = data_dict\n","        self.item_ids = list(data_dict.keys())\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.item_ids)\n","\n","    def __getitem__(self, idx):\n","        item_id = self.item_ids[idx]\n","        item_data = self.data_dict[item_id]\n","\n","        image_paths = item_data['images']\n","        label = item_data['label']\n","\n","        # load all images for the bag\n","        instances = []\n","        for img_path in image_paths:\n","            try:\n","                img = Image.open(img_path).convert('RGB')\n","                if self.transform:\n","                    img = self.transform(img)\n","                instances.append(img)\n","            except Exception as e:\n","                print(f\"Error loading {img_path}: {e}\")\n","\n","        # handle empty bags\n","        if len(instances) == 0:\n","            instances = [torch.zeros(3, 224, 224)]\n","\n","        instances = torch.stack(instances)\n","        return item_id, instances, torch.tensor(label, dtype=torch.long)\n","\n","# custom collate function for dataset\n","def collate_mil_bags(batch):\n","    bag_ids, instances, labels = zip(*batch)\n","    bag_dict = {bid: inst for bid, inst in zip(bag_ids, instances)}\n","    labels = torch.stack(labels)\n","    return bag_dict, labels\n"]},{"cell_type":"code","execution_count":null,"id":"3612fec7","metadata":{"id":"3612fec7"},"outputs":[],"source":["def pad_to_square(img, fill=(255, 255, 255)):\n","    w, h = img.size\n","    if w == h:\n","        return img\n","    diff = abs(h - w)\n","    if w < h:\n","        padding = (diff // 2, 0, diff - diff // 2, 0)\n","    else:\n","        padding = (0, diff // 2, 0, diff - diff // 2)\n","    return transforms.functional.pad(img, padding, fill=fill)\n"]},{"cell_type":"code","execution_count":null,"id":"85b989f4","metadata":{"id":"85b989f4"},"outputs":[],"source":["def create_data_dict_from_csv(csv_path, images_folder):\n","    df = pd.read_csv(csv_path)\n","\n","    # create label mapping\n","    unique_labels = sorted(df['label'].unique())\n","    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n","    reverse_mapping = {idx: label for label, idx in label_mapping.items()}\n","\n","    print(f\"Found {len(unique_labels)} fabric classes:\")\n","    for label, idx in label_mapping.items():\n","        print(f\"  {idx}: {label}\")\n","\n","    images_path = Path(images_folder)\n","\n","    data_dict = {}\n","    missing_images = []\n","\n","    for item_id, group in df.groupby('item_id'):\n","        image_paths = []\n","        for image_id in group['image_id']:\n","            img_path = images_path / f\"{image_id}.jpg\"\n","            if img_path.exists():\n","                image_paths.append(str(img_path))\n","            else:\n","                missing_images.append(f\"{image_id}.jpg\")\n","\n","        # skip items with no valid images\n","        if len(image_paths) == 0:\n","            print(f\"Warning: Item {item_id} has no valid images, skipping...\")\n","            continue\n","\n","        # get label\n","        label_name = group['label'].iloc[0]\n","        label_idx = label_mapping[label_name]\n","\n","        data_dict[f'item_{item_id}'] = {\n","            'images': image_paths,\n","            'label': label_idx\n","        }\n","\n","    if missing_images:\n","        print(f\"\\ {len(missing_images)} images not found in {images_folder}\")\n","        print(f\"Missing: {missing_images[:5]}\")\n","\n","    print(f\"\\nCreated data dictionary with {len(data_dict)} items\")\n","    if len(data_dict) > 0:\n","        example_key = list(data_dict.keys())[0]\n","        print(f\"Example item: {example_key}\")\n","        print(f\"  - Images: {len(data_dict[example_key]['images'])}\")\n","        print(f\"  - Label: {data_dict[example_key]['label']} ({reverse_mapping[data_dict[example_key]['label']]})\")\n","\n","    return data_dict, label_mapping, reverse_mapping"]},{"cell_type":"code","execution_count":null,"id":"afd8a741","metadata":{"id":"afd8a741","outputId":"6728d7a4-f719-4e2c-d6de-bc638b9fc0a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7 fabric classes:\n","  0: Acrylic\n","  1: Cotton\n","  2: Linen\n","  3: Nylon\n","  4: Polyester\n","  5: Suede\n","  6: Viscose\n","\n","Created data dictionary with 2145 items\n","Example item: item_1\n","  - Images: 6\n","  - Label: 0 (Acrylic)\n","{'images': ['D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\1.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\2.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\3.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\4.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\5.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\6.jpg'], 'label': 0}\n","{'Acrylic': 0, 'Cotton': 1, 'Linen': 2, 'Nylon': 3, 'Polyester': 4, 'Suede': 5, 'Viscose': 6}\n"]}],"source":["csv_path = \"D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_data.csv\"\n","images_folder = \"D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\"\n","data_dict, label_mapping, reverse_mapping = create_data_dict_from_csv(csv_path, images_folder)\n","\n","print(label_mapping)"]},{"cell_type":"code","execution_count":null,"id":"be37f5e4","metadata":{"id":"be37f5e4","outputId":"9fbaab66-6cde-43fb-995d-2b901f03481f"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'images': ['D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\1.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\2.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\3.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\4.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\5.jpg', 'D:\\\\csci_461_textiles_project\\\\data\\\\fiber\\\\fiber_images\\\\6.jpg'], 'label': 0}\n"]}],"source":["print(data_dict.get('item_1'))\n"]},{"cell_type":"code","execution_count":null,"id":"ca6f25a3","metadata":{"id":"ca6f25a3","outputId":"67c0329d-aa3d-4723-8276-f48d549cdefa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Counter({1: 200, 2: 200, 5: 154, 6: 101, 3: 88, 4: 61, 0: 22})\n"]},{"data":{"text/plain":["826"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import random\n","random.seed(42)\n","\n","# target maximum per class\n","max_per_class = {\n","    1: 200,  # downsample class 1\n","    2: 200,  # downsample class 2\n","}\n","\n","# group items by label\n","by_class = {}\n","for k, v in data_dict.items():\n","    lbl = v[\"label\"]\n","    by_class.setdefault(lbl, []).append((k, v))\n","\n","# downsample large classes\n","balanced_items = []\n","for lbl, items in by_class.items():\n","    if lbl in max_per_class and len(items) > max_per_class[lbl]:\n","        items = random.sample(items, max_per_class[lbl])\n","    balanced_items.extend(items)\n","\n","# reconstruct new data_dict\n","balanced_data_dict = {k: v for k, v in balanced_items}\n","\n","# check new distribution\n","from collections import Counter\n","print(Counter(v[\"label\"] for v in balanced_data_dict.values()))\n","data_dict = balanced_data_dict\n","len(data_dict)\n"]},{"cell_type":"code","execution_count":null,"id":"44cbe291","metadata":{"id":"44cbe291"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","from PIL import Image\n","from pathlib import Path\n","import pandas as pd\n","from tqdm import tqdm\n","\n","\n","def load_image(path, transform):\n","    img = Image.open(path).convert(\"RGB\")\n","    return transform(img)\n","\n","\n","def run_resnet_baseline_bagging(\n","    data_dict,\n","    pretrained_checkpoint_path,\n","    textilenet_num_classes=33,\n","    save_dir=\"textilenet_image_baseline\",\n","):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Running TextileNet image-level baseline on:\", device)\n","\n","    save_path = Path(save_dir)\n","    save_path.mkdir(exist_ok=True, parents=True)\n","\n","    # load model\n","    model = resnet18(num_classes=textilenet_num_classes)\n","\n","    ckpt = torch.load(pretrained_checkpoint_path, map_location=device)\n","\n","    if \"model\" in ckpt:\n","        print(\"✓ Found 'model' inside checkpoint — using ckpt['model']\")\n","        state_dict = ckpt[\"model\"]\n","    else:\n","        state_dict = ckpt\n","\n","    clean_state = {}\n","    for k, v in state_dict.items():\n","        clean_k = k.replace(\"module.\", \"\")\n","        clean_state[clean_k] = v\n","\n","    print(\"✓ Stripped 'module.' prefix from state_dict keys\")\n","\n","    model.load_state_dict(clean_state)\n","    model.to(device)\n","    model.eval()\n","\n","    print(\"✓ Loaded TextileNet ResNet18 checkpoint\")\n","\n","    transform = transforms.Compose([\n","        pad_to_square,\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","\n","    rows = []\n","\n","    for item_id, item in tqdm(data_dict.items(), desc=\"Processing bags\"):\n","\n","        image_paths = item[\"images\"]\n","        true_lbl = item[\"label\"]\n","\n","        image_probs = []\n","\n","        for img_path in image_paths:\n","            img_tensor = load_image(img_path, transform).unsqueeze(0).to(device)\n","\n","            with torch.no_grad():\n","                logits = model(img_tensor)\n","                probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n","\n","            image_probs.append(probs)\n","\n","        image_probs = np.array(image_probs)\n","        bag_probs = image_probs.mean(axis=0)\n","\n","        bag_pred = int(bag_probs.argmax())\n","\n","        row = {\n","            \"bag_id\": item_id,\n","            \"true_label\": true_lbl,\n","            \"predicted_label\": bag_pred,\n","        }\n","\n","        for c in range(textilenet_num_classes):\n","            row[f\"prob_class_{c}\"] = float(bag_probs[c])\n","\n","        rows.append(row)\n","\n","    df = pd.DataFrame(rows)\n","    out_file = save_path / \"textilenet_bag_predictions.csv\"\n","    df.to_csv(out_file, index=False)\n","\n","    print(\"✓ Saved:\", out_file)\n","    return df\n"]},{"cell_type":"code","execution_count":null,"id":"68ffc60c","metadata":{"id":"68ffc60c","outputId":"492af89a-c652-4241-ed74-82653a89902a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running TextileNet image-level baseline on: cpu\n","✓ Found 'model' inside checkpoint — using ckpt['model']\n","✓ Stripped 'module.' prefix from state_dict keys\n","✓ Loaded TextileNet ResNet18 checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["Processing bags: 100%|██████████| 826/826 [06:12<00:00,  2.22it/s]"]},{"name":"stdout","output_type":"stream","text":["✓ Saved: D:\\csci_461_textiles_project\\fiber_resnet_test_three\\image_baseline\\textilenet_bag_predictions.csv\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["results = run_resnet_baseline_bagging(\n","    data_dict=data_dict,\n","    pretrained_checkpoint_path=\"D:/csci_461_textiles_project/res18_ckpt.pth\",\n","    textilenet_num_classes=33,\n","    save_dir=\"D:/csci_461_textiles_project/fiber_resnet_test_three/image_baseline\"\n",")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}